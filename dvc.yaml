stages:
  llm_generate:
    cmd: poetry run python -m run.llm_generate --prediction_target_name ${generate.generate_data_name} --prompt_name ${generate.generate_prompt_name}
    deps:
      - data/generate_target/${generate.generate_data_name}.csv
      - data/src/prompt/${generate.generate_prompt_name}.txt
      - run/llm_generate.py
    outs:
      - data/submit/
    params:
      - generate

  rulebase_evaluate:
    foreach: ${rulebase_funcs}
    do: 
      cmd: python -m run.rulebase_evaluate --rulebase_func_name ${item} --submit_file_name ${submit_file_name}
      deps:
        - data/submit/${submit_file_name}.csv
        - src/judge/rulebase_funcs/${item}.py
        - run/rulebase_evaluate.py
      outs:
        - data/result/rulebase_${submit_file_name}_${item}.json

  llm_evaluate:
    foreach: ${prompts}
    do: 
      cmd: python -m run.llm_evaluate --prompt_name ${item} --submit_file_name ${submit_file_name}
      deps:
        - data/submit/${submit_file_name}.csv
        - src/judge/prompt/${item}.txt
        - run/llm_evaluate.py
      outs:
        - data/result/llm_${submit_file_name}_${item}.json
  make_dashboard:
    cmd: poetry run python -m run.make_dashboard
    deps:
      - data/result/
      - run/make_dashboard.py
    outs:
      - data/dashboard/mikoto_run_ids.csv
      - data/dashboard/scores.csv
  